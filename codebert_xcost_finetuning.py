# -*- coding: utf-8 -*-
"""codebert-xcost-finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pCqxCwNcC9vLbFe5zES373QWFu0lRgQm
"""



from transformers import RobertaTokenizer, RobertaModel


def get_model():
    model = RobertaModel.from_pretrained('microsoft/codebert-base')
    tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')
    model = model.eval().cuda()
    return model, tokenizer

model, tokenizer = get_model()

import sys
from code_search import get_dataset, collate_fn, ContrastiveTrainer

languages = ["C", "PHP", "Java", "C++", "C#", "Javascript", "Python"]
root_path = "XLCoST_data"


dataset = get_dataset(root_path=root_path, languages=languages)

dataset

from transformers import TrainingArguments


training_args = TrainingArguments(
    "contrastive_trainer",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_steps=200,
    num_train_epochs=1,
    evaluation_strategy="no",
    report_to="none",
    remove_unused_columns=False,
    warmup_steps=1000,
    save_strategy="epoch"
)
trainer = ContrastiveTrainer(
    model,
    training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["val"],
    data_collator=lambda x: collate_fn(x, tokenizer),
)
trainer.train()

