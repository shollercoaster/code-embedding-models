# -*- coding: utf-8 -*-
"""graphcodebert-csn-finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pX6v6qpL5uvsCIRh9siPPuDh0z7kvmHb
"""

import os
from transformers import Trainer, TrainingArguments

import torch
from datasets import Dataset, DatasetDict
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer
from torch.utils.data import DataLoader
from peft import LoraConfig

from transformers import RobertaTokenizer, RobertaModel

from search_utils import load_jsonl, CustomDataset 

languages = ['ruby', 'go', 'php', 'python', 'java', 'javascript']
root_path = "../../dataset/CSN"

def get_dataset(root_path, languages, split):
    for lang in languages:
        data_path = os.path.join(root_path, lang, f"{split}.jsonl")
        data_list = load_jsonl(data_path)

    # merged_examples = []
    # for lang, examples in data_list.items():
        # merged_examples.extend(examples)

    torch_dataset = CustomDataset(data_list)

    return torch_dataset

def get_model():
    model = AutoModel.from_pretrained('microsoft/graphcodebert-base')
    tokenizer = AutoTokenizer.from_pretrained('microsoft/graphcodebert-base')
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["query", "value"],
        lora_dropout=0.1
    )
    
    # model = get_peft_model(model, lora_config)
    model.add_adapter(lora_config, adapter_name="graphcodebert-text2code-lora-r16")
    model.set_adapter("graphcodebert-text2code-lora-r16")
    return model, tokenizer

def collate_fn(batch, tokenizer):
    anchor_codes = [item[0] for item in batch]
    positive_codes = [item[1] for item in batch]

    anchor_codes_tensor = tokenizer(
        anchor_codes,
        truncation=True,
        max_length=512,
        padding="longest",
        return_tensors="pt",
    )
    positive_codes_tensor = tokenizer(
        positive_codes,
        truncation=True,
        max_length=512,
        padding="longest",
        return_tensors="pt",
    )

    collated_batch = {
        "anchor": anchor_codes_tensor,
        "positive": positive_codes_tensor,
        "labels": torch.tensor(
            range(len(anchor_codes)), dtype=torch.long
        ),
    }

    return collated_batch

def _get_pooled_embeds(model, batch, field):
    ids = batch[field]["input_ids"]
    mask = batch[field]["attention_mask"]
    embeds = model(ids, attention_mask=mask)[0]
    in_mask = mask.unsqueeze(-1).expand(embeds.size()).float()
    pooled_embeds = torch.sum(embeds * in_mask, 1) / torch.clamp(
        in_mask.sum(1), min=1e-6
    )
    # print(field, ": ", pooled_embeds.shape)
    return pooled_embeds

class ContrastiveTrainer(Trainer):

    def compute_loss(self, model, batch, return_outputs=False):
        a = _get_pooled_embeds(model, batch, field="anchor")
        p = _get_pooled_embeds(model, batch, field="positive")
        # assert a.shape == (16, 768)
        # assert p.shape == (16, 768)
        scores = torch.stack(
            [F.cosine_similarity(a_i.reshape(1, a_i.shape[0]), p, eps=1e-6) for a_i in a]
        )
        # assert scores.shape == (16,16)
        loss = F.cross_entropy(scores * 5, batch["labels"])
        return (loss, scores) if return_outputs else loss

def run(model, tokenizer):
    training_args = TrainingArguments(
        "contrastive_trainer",
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        logging_steps=400,
        num_train_epochs=1,
        evaluation_strategy="no",
        report_to="none",
        remove_unused_columns=False,
        warmup_steps=4000,
        save_strategy="epoch"
    )
    trainer = ContrastiveTrainer(
        model,
        training_args,
        train_dataset=get_dataset(root_path=root_path, languages=languages, split="train"),
        # eval_dataset=get_dataset(root_path=root_path, languages=languages, split="valid"),
        data_collator=lambda x: collate_fn(x, tokenizer),
    )
    trainer.train()

model, tokenizer = get_model()

run(model, tokenizer)

model.push_to_hub("graphcodebert-text2code-lora-r16")
